/* tslint:disable */
/* eslint-disable */
/**
* Read a Parquet file into Arrow data using the [`arrow2`](https://crates.io/crates/arrow2) and
* [`parquet2`](https://crates.io/crates/parquet2) Rust crates.
*
* This returns an Arrow table in WebAssembly memory. To transfer the Arrow table to JavaScript
* memory you have two options:
*
* - (Easier): Call {@linkcode Table.intoIPCStream} to construct a buffer that can be parsed with
*   Arrow JS's `tableFromIPC` function.
* - (More performant but bleeding edge): Call {@linkcode Table.intoFFI} to construct a data
*   representation that can be parsed zero-copy from WebAssembly with
*   [arrow-js-ffi](https://github.com/kylebarron/arrow-js-ffi).
*
* Example:
*
* ```js
* import { tableFromIPC } from "apache-arrow";
* // Edit the `parquet-wasm` import as necessary
* import { readParquet } from "parquet-wasm/node/arrow2";
*
* const resp = await fetch("https://example.com/file.parquet");
* const parquetUint8Array = new Uint8Array(await resp.arrayBuffer());
* const arrowWasmTable = readParquet(parquetUint8Array);
* const arrowTable = tableFromIPC(arrowWasmTable.intoIPCStream());
* ```
*
* @param parquet_file Uint8Array containing Parquet data
* @param {Uint8Array} parquet_file
* @returns {Table}
*/
export function readParquet(parquet_file: Uint8Array): Table;
/**
* Read metadata from a Parquet file using the [`arrow2`](https://crates.io/crates/arrow2) and
* [`parquet2`](https://crates.io/crates/parquet2) Rust crates.
*
* Example:
*
* ```js
* // Edit the `parquet-wasm` import as necessary
* import { readMetadata } from "parquet-wasm/node/arrow2";
*
* const resp = await fetch("https://example.com/file.parquet");
* const parquetUint8Array = new Uint8Array(await resp.arrayBuffer());
* const parquetFileMetaData = readMetadata(parquetUint8Array);
* ```
*
* @param parquet_file Uint8Array containing Parquet data
* @returns a {@linkcode FileMetaData} object containing metadata of the Parquet file.
* @param {Uint8Array} parquet_file
* @returns {FileMetaData}
*/
export function readMetadata(parquet_file: Uint8Array): FileMetaData;
/**
* Read a single row group from a Parquet file into Arrow data using the
* [`arrow2`](https://crates.io/crates/arrow2) and [`parquet2`](https://crates.io/crates/parquet2)
* Rust crates.
*
* This returns an Arrow record batch in WebAssembly memory. To transfer the Arrow record batch to
* JavaScript memory you have two options:
*
* - (Easier): Call {@linkcode RecordBatch.intoIPCStream} to construct a buffer that can be parsed
*   with Arrow JS's `tableFromIPC` function.
* - (More performant but bleeding edge): Call {@linkcode RecordBatch.intoFFI} to construct a data
*   representation that can be parsed zero-copy from WebAssembly with
*   [arrow-js-ffi](https://github.com/kylebarron/arrow-js-ffi).
*
* Example:
*
* ```js
* import { tableFromIPC } from "apache-arrow";
* // Edit the `parquet-wasm` import as necessary
* import { readRowGroup, readMetadata } from "parquet-wasm/node/arrow2";
*
* const resp = await fetch("https://example.com/file.parquet");
* const parquetUint8Array = new Uint8Array(await resp.arrayBuffer());
* const parquetFileMetaData = readMetadata(parquetUint8Array);
*
* const arrowSchema = parquetFileMetaData.arrowSchema();
* // Read only the first row group
* const parquetRowGroupMeta = parquetFileMetaData.rowGroup(0);
*
* // Read only the first row group
* const arrowWasmBatch = readRowGroup(
*   parquetUint8Array,
*   arrowSchema,
*   parquetRowGroupMeta
* );
* const arrowJsTable = tableFromIPC(arrowWasmBatch.intoIPCStream());
* // This table will only have one batch
* const arrowJsRecordBatch = arrowJsTable.batches[0];
* ```
*
* Note that you can get the number of row groups in a Parquet file using {@linkcode FileMetaData.numRowGroups}
*
* @param parquet_file Uint8Array containing Parquet data
* @param schema Use {@linkcode FileMetaData.arrowSchema} to create.
* @param meta {@linkcode RowGroupMetaData} from a call to {@linkcode readMetadata}
* @param {Uint8Array} parquet_file
* @param {ArrowSchema} schema
* @param {RowGroupMetaData} meta
* @returns {RecordBatch}
*/
export function readRowGroup(parquet_file: Uint8Array, schema: ArrowSchema, meta: RowGroupMetaData): RecordBatch;
/**
* Asynchronously read metadata from a Parquet file using the
* [`arrow2`](https://crates.io/crates/arrow2) and [`parquet2`](https://crates.io/crates/parquet2)
* Rust crates.
*
* For now, this requires knowing the content length of the file, but hopefully this will be
* relaxed in the future. If you don't know the contentLength of the file, this will perform a
* HEAD request to do so.
*
* Example:
*
* ```js
* // Edit the `parquet-wasm` import as necessary
* import { readMetadataAsync } from "parquet-wasm";
*
* const parquetFileMetaData = await readMetadataAsync(url, contentLength);
* ```
*
* @param url String location of remote Parquet file containing Parquet data
* @param content_length Number content length of file in bytes
* @returns a {@linkcode FileMetaData} object containing metadata of the Parquet file.
* @param {string} url
* @param {number | undefined} content_length
* @returns {Promise<FileMetaData>}
*/
export function readMetadataAsync(url: string, content_length?: number): Promise<FileMetaData>;
/**
* Asynchronously read a single row group from a Parquet file into Arrow data using the
* [`arrow2`](https://crates.io/crates/arrow2) and [`parquet2`](https://crates.io/crates/parquet2)
* Rust crates.
*
* Example:
*
* ```ts
* import * as arrowJs from "apache-arrow";
* // Edit the `parquet-wasm` import as necessary
* import {
*   readRowGroupAsync,
*   readMetadataAsync,
*   RecordBatch,
* } from "parquet-wasm/node/arrow2";
*
* const url = "https://example.com/file.parquet";
* const headResp = await fetch(url, { method: "HEAD" });
* const length = parseInt(headResp.headers.get("Content-Length"));
*
* const parquetFileMetaData = await readMetadataAsync(url, length);
* const arrowSchema = parquetFileMetaData.arrowSchema();
*
* // Read all batches from the file in parallel
* const promises: Promise<RecordBatch>[] = [];
* for (let i = 0; i < parquetFileMetaData.numRowGroups(); i++) {
*   const rowGroupMeta = parquetFileMetaData.rowGroup(i);
*   const rowGroupPromise = readRowGroupAsync(url, rowGroupMeta, arrowSchema);
*   promises.push(rowGroupPromise);
* }
*
* // Collect the per-batch requests
* const wasmRecordBatchChunks = await Promise.all(promises);
*
* // Parse the wasm record batches into JS record batches
* const jsRecordBatchChunks: arrowJs.RecordBatch[] = [];
* for (const wasmRecordBatch of wasmRecordBatchChunks) {
*   const arrowJsTable = arrowJs.tableFromIPC(wasmRecordBatch.intoIPCStream());
*   // This should never throw
*   if (arrowJsTable.batches.length > 1) throw new Error();
*   const arrowJsRecordBatch = arrowJsTable.batches[0];
*   jsRecordBatchChunks.push(arrowJsRecordBatch);
* }
*
* // Concatenate the JS record batches into a table
* const jsTable = new arrowJs.Table(recordBatchChunks);
* ```
*
* Note that you can get the number of row groups in a Parquet file using {@linkcode FileMetaData.numRowGroups}
*
* @param url String location of remote Parquet file containing Parquet data
* @param schema Use {@linkcode FileMetaData.arrowSchema} to create.
* @param meta {@linkcode RowGroupMetaData} from a call to {@linkcode readMetadataAsync}
* @param {string} url
* @param {RowGroupMetaData} row_group_meta
* @param {ArrowSchema} arrow_schema
* @returns {Promise<RecordBatch>}
*/
export function readRowGroupAsync(url: string, row_group_meta: RowGroupMetaData, arrow_schema: ArrowSchema): Promise<RecordBatch>;
/**
* Write Arrow data to a Parquet file using the [`arrow2`](https://crates.io/crates/arrow2) and
* [`parquet2`](https://crates.io/crates/parquet2) Rust crates.
*
* For example, to create a Parquet file with Snappy compression:
*
* ```js
* import { tableToIPC } from "apache-arrow";
* // Edit the `parquet-wasm` import as necessary
* import {
*   Table,
*   WriterPropertiesBuilder,
*   Compression,
*   writeParquet,
* } from "parquet-wasm/node/arrow2";
*
* // Given an existing arrow JS table under `table`
* const wasmTable = Table.fromIPCStream(tableToIPC(table, "stream"));
* const writerProperties = new WriterPropertiesBuilder()
*   .setCompression(Compression.SNAPPY)
*   .build();
* const parquetUint8Array = writeParquet(wasmTable, writerProperties);
* ```
*
* If `writerProperties` is not provided or is `null`, the default writer properties will be used.
* This is equivalent to `new WriterPropertiesBuilder().build()`.
*
* @param table A {@linkcode Table} representation in WebAssembly memory.
* @param writer_properties (optional) Configuration for writing to Parquet. Use the {@linkcode
*   WriterPropertiesBuilder} to build a writing configuration, then call `.build()` to create an
*   immutable writer properties to pass in here.
* @returns Uint8Array containing written Parquet data.
* @param {Table} table
* @param {WriterProperties | undefined} writer_properties
* @returns {Uint8Array}
*/
export function writeParquet(table: Table, writer_properties?: WriterProperties): Uint8Array;
/**
* @param {string} url
* @returns {Promise<ReadableStream>}
*/
export function readParquetStream(url: string): Promise<ReadableStream>;
/**
* Returns a handle to this wasm instance's `WebAssembly.Memory`
* @returns {any}
*/
export function wasmMemory(): any;
/**
* Returns a handle to this wasm instance's `WebAssembly.Table` which is the indirect function
* table used by Rust
* @returns {any}
*/
export function _functionTable(): any;
/**
* Supported compression algorithms.
*
* Codecs added in format version X.Y can be read by readers based on X.Y and later.
* Codec support may vary between readers based on the format version and
* libraries available at runtime.
*/
export enum Compression {
  UNCOMPRESSED = 0,
  SNAPPY = 1,
  GZIP = 2,
  BROTLI = 3,
/**
* @deprecated as of Parquet 2.9.0.
* Switch to LZ4_RAW
*/
  LZ4 = 4,
  ZSTD = 5,
  LZ4_RAW = 6,
}
/**
* Encodings supported by Parquet.
* Not all encodings are valid for all types. These enums are also used to specify the
* encoding of definition and repetition levels.
*/
export enum Encoding {
/**
* Default byte encoding.
* - BOOLEAN - 1 bit per value, 0 is false; 1 is true.
* - INT32 - 4 bytes per value, stored as little-endian.
* - INT64 - 8 bytes per value, stored as little-endian.
* - FLOAT - 4 bytes per value, stored as little-endian.
* - DOUBLE - 8 bytes per value, stored as little-endian.
* - BYTE_ARRAY - 4 byte length stored as little endian, followed by bytes.
* - FIXED_LEN_BYTE_ARRAY - just the bytes are stored.
*/
  PLAIN = 0,
/**
* **Deprecated** dictionary encoding.
*
* The values in the dictionary are encoded using PLAIN encoding.
* Since it is deprecated, RLE_DICTIONARY encoding is used for a data page, and
* PLAIN encoding is used for dictionary page.
*/
  PLAIN_DICTIONARY = 1,
/**
* Group packed run length encoding.
*
* Usable for definition/repetition levels encoding and boolean values.
*/
  RLE = 2,
/**
* Bit packed encoding.
*
* This can only be used if the data has a known max width.
* Usable for definition/repetition levels encoding.
*/
  BIT_PACKED = 3,
/**
* Delta encoding for integers, either INT32 or INT64.
*
* Works best on sorted data.
*/
  DELTA_BINARY_PACKED = 4,
/**
* Encoding for byte arrays to separate the length values and the data.
*
* The lengths are encoded using DELTA_BINARY_PACKED encoding.
*/
  DELTA_LENGTH_BYTE_ARRAY = 5,
/**
* Incremental encoding for byte arrays.
*
* Prefix lengths are encoded using DELTA_BINARY_PACKED encoding.
* Suffixes are stored using DELTA_LENGTH_BYTE_ARRAY encoding.
*/
  DELTA_BYTE_ARRAY = 6,
/**
* Dictionary encoding.
*
* The ids are encoded using the RLE encoding.
*/
  RLE_DICTIONARY = 7,
/**
* Encoding for floating-point data.
*
* K byte-streams are created where K is the size in bytes of the data type.
* The individual bytes of an FP value are scattered to the corresponding stream and
* the streams are concatenated.
* This itself does not reduce the size of the data but can lead to better compression
* afterwards.
*/
  BYTE_STREAM_SPLIT = 8,
}
/**
* The Parquet version to use when writing
*/
export enum WriterVersion {
  V1 = 0,
  V2 = 1,
}
/**
* Arrow Schema representing a Parquet file.
*/
export class ArrowSchema {
  free(): void;
/**
* Clone this struct in wasm memory.
* @returns {ArrowSchema}
*/
  copy(): ArrowSchema;
}
/**
* Metadata for a column chunk.
*/
export class ColumnChunkMetaData {
  free(): void;
/**
* File where the column chunk is stored.
*
* If not set, assumed to belong to the same file as the metadata.
* This path is relative to the current file.
* @returns {string | undefined}
*/
  filePath(): string | undefined;
/**
* Byte offset in `file_path()`.
* @returns {bigint}
*/
  fileOffset(): bigint;
/**
* @returns {string}
*/
  pathInSchema(): string;
/**
* @returns {boolean}
*/
  statistics_exist(): boolean;
/**
* @returns {any}
*/
  getStatisticsMinValue(): any;
/**
* @returns {any}
*/
  getStatisticsMaxValue(): any;
/**
* @returns {any}
*/
  getStatisticsNullCount(): any;
/**
* Total number of values in this column chunk. Note that this is not necessarily the number
* of rows. E.g. the (nested) array `[[1, 2], [3]]` has 2 rows and 3 values.
* @returns {bigint}
*/
  numValues(): bigint;
/**
* Returns the total compressed data size of this column chunk.
* @returns {bigint}
*/
  compressedSize(): bigint;
/**
* Returns the total uncompressed data size of this column chunk.
* @returns {bigint}
*/
  uncompressedSize(): bigint;
/**
* Returns the offset for the column data.
* @returns {bigint}
*/
  dataPageOffset(): bigint;
/**
* Returns `true` if this column chunk contains a index page, `false` otherwise.
* @returns {boolean}
*/
  hasIndexPage(): boolean;
/**
* Returns the offset for the index page.
* @returns {bigint | undefined}
*/
  indexPageOffset(): bigint | undefined;
/**
* Returns the offset for the dictionary page, if any.
* @returns {bigint | undefined}
*/
  dictionaryPageOffset(): bigint | undefined;
/**
* Returns the number of encodings for this column
* @returns {number}
*/
  numColumnEncodings(): number;
/**
* Returns the offset and length in bytes of the column chunk within the file
* @returns {BigUint64Array}
*/
  byteRange(): BigUint64Array;
}
/**
* The set of supported logical types in this crate.
*/
export class DataType {
  free(): void;
}
/**
* A representation of an Arrow RecordBatch in WebAssembly memory exposed as FFI-compatible
* structs through the Arrow C Data Interface.
*/
export class FFIRecordBatch {
  free(): void;
/**
* Access the pointer to the
* [`ArrowArray`](https://arrow.apache.org/docs/format/CDataInterface.html#structure-definitions)
* struct. This can be viewed or copied (without serialization) to an Arrow JS `RecordBatch` by
* using [`arrow-js-ffi`](https://github.com/kylebarron/arrow-js-ffi). You can access the
* [`WebAssembly.Memory`](https://developer.mozilla.org/en-US/docs/WebAssembly/JavaScript_interface/Memory)
* instance by using {@linkcode wasmMemory}.
*
* **Example**:
*
* ```ts
* import { parseRecordBatch } from "arrow-js-ffi";
*
* const wasmRecordBatch: FFIRecordBatch = ...
* const wasmMemory: WebAssembly.Memory = wasmMemory();
*
* // Pass `true` to copy arrays across the boundary instead of creating views.
* const jsRecordBatch = parseRecordBatch(
*   wasmMemory.buffer,
*   wasmRecordBatch.arrayAddr(),
*   wasmRecordBatch.schemaAddr(),
*   true
* );
* ```
* @returns {number}
*/
  arrayAddr(): number;
/**
* Access the pointer to the
* [`ArrowSchema`](https://arrow.apache.org/docs/format/CDataInterface.html#structure-definitions)
* struct. This can be viewed or copied (without serialization) to an Arrow JS `Field` by
* using [`arrow-js-ffi`](https://github.com/kylebarron/arrow-js-ffi). You can access the
* [`WebAssembly.Memory`](https://developer.mozilla.org/en-US/docs/WebAssembly/JavaScript_interface/Memory)
* instance by using {@linkcode wasmMemory}.
*
* **Example**:
*
* ```ts
* import { parseRecordBatch } from "arrow-js-ffi";
*
* const wasmRecordBatch: FFIRecordBatch = ...
* const wasmMemory: WebAssembly.Memory = wasmMemory();
*
* // Pass `true` to copy arrays across the boundary instead of creating views.
* const jsRecordBatch = parseRecordBatch(
*   wasmMemory.buffer,
*   wasmRecordBatch.arrayAddr(),
*   wasmRecordBatch.schemaAddr(),
*   true
* );
* ```
* @returns {number}
*/
  schemaAddr(): number;
}
/**
* A representation of an Arrow Table in WebAssembly memory exposed as FFI-compatible
* structs through the Arrow C Data Interface.
*/
export class FFITable {
  free(): void;
/**
* Get the total number of record batches in the table
* @returns {number}
*/
  numBatches(): number;
/**
* Get the pointer to one ArrowSchema FFI struct
* @returns {number}
*/
  schemaAddr(): number;
/**
* Get the pointer to one ArrowArray FFI struct for a given chunk index and column index
*
* Access the pointer to one
* [`ArrowArray`](https://arrow.apache.org/docs/format/CDataInterface.html#structure-definitions)
* struct representing one of the internal `RecordBatch`es. This can be viewed or copied (without serialization) to an Arrow JS `RecordBatch` by
* using [`arrow-js-ffi`](https://github.com/kylebarron/arrow-js-ffi). You can access the
* [`WebAssembly.Memory`](https://developer.mozilla.org/en-US/docs/WebAssembly/JavaScript_interface/Memory)
* instance by using {@linkcode wasmMemory}.
*
* **Example**:
*
* ```ts
* import * as arrow from "apache-arrow";
* import { parseRecordBatch } from "arrow-js-ffi";
*
* const wasmTable: FFITable = ...
* const wasmMemory: WebAssembly.Memory = wasmMemory();
*
* const jsBatches: arrow.RecordBatch[] = []
* for (let i = 0; i < wasmTable.numBatches(); i++) {
*   // Pass `true` to copy arrays across the boundary instead of creating views.
*   const jsRecordBatch = parseRecordBatch(
*     wasmMemory.buffer,
*     wasmTable.arrayAddr(i),
*     wasmTable.schemaAddr(),
*     true
*   );
*   jsBatches.push(jsRecordBatch);
* }
* const jsTable = new arrow.Table(jsBatches);
* ```
*
* @param chunk number The chunk index to use
* @returns number pointer to an ArrowArray FFI struct in Wasm memory
* @param {number} chunk
* @returns {number}
*/
  arrayAddr(chunk: number): number;
/**
*/
  drop(): void;
}
/**
*/
export class FFIVector {
  free(): void;
/**
* @returns {number}
*/
  array_addr(): number;
/**
* @returns {number}
*/
  field_addr(): number;
}
/**
* Metadata for a Parquet file.
*/
export class FileMetaData {
  free(): void;
/**
* Clone this struct in wasm memory.
* @returns {FileMetaData}
*/
  copy(): FileMetaData;
/**
* Version of this file.
* @returns {number}
*/
  version(): number;
/**
* number of rows in the file.
* @returns {number}
*/
  numRows(): number;
/**
* String message for application that wrote this file.
* @returns {string | undefined}
*/
  createdBy(): string | undefined;
/**
* Number of row groups in the file
* @returns {number}
*/
  numRowGroups(): number;
/**
* Returns a single RowGroupMetaData by index
* @param {number} i
* @returns {RowGroupMetaData}
*/
  rowGroup(i: number): RowGroupMetaData;
/**
* @returns {SchemaDescriptor}
*/
  schema(): SchemaDescriptor;
/**
* @returns {any}
*/
  keyValueMetadata(): any;
/**
* @returns {ArrowSchema}
*/
  arrowSchema(): ArrowSchema;
}
/**
*/
export class IntoUnderlyingByteSource {
  free(): void;
/**
* @param {any} controller
*/
  start(controller: any): void;
/**
* @param {any} controller
* @returns {Promise<any>}
*/
  pull(controller: any): Promise<any>;
/**
*/
  cancel(): void;
/**
*/
  readonly autoAllocateChunkSize: number;
/**
*/
  readonly type: string;
}
/**
*/
export class IntoUnderlyingSink {
  free(): void;
/**
* @param {any} chunk
* @returns {Promise<any>}
*/
  write(chunk: any): Promise<any>;
/**
* @returns {Promise<any>}
*/
  close(): Promise<any>;
/**
* @param {any} reason
* @returns {Promise<any>}
*/
  abort(reason: any): Promise<any>;
}
/**
*/
export class IntoUnderlyingSource {
  free(): void;
/**
* @param {any} controller
* @returns {Promise<any>}
*/
  pull(controller: any): Promise<any>;
/**
*/
  cancel(): void;
}
/**
* Raw options for [`pipeTo()`](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/pipeTo).
*/
export class PipeOptions {
  free(): void;
/**
*/
  readonly preventAbort: boolean;
/**
*/
  readonly preventCancel: boolean;
/**
*/
  readonly preventClose: boolean;
/**
*/
  readonly signal: AbortSignal | undefined;
}
/**
*/
export class QueuingStrategy {
  free(): void;
/**
*/
  readonly highWaterMark: number;
}
/**
* Raw options for [`getReader()`](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/getReader).
*/
export class ReadableStreamGetReaderOptions {
  free(): void;
/**
*/
  readonly mode: any;
}
/**
* A group of columns of equal length in WebAssembly memory with an associated {@linkcode Schema}.
*/
export class RecordBatch {
  free(): void;
/**
* Get a column's vector by index.
* @param {number} index
* @returns {Vector | undefined}
*/
  column(index: number): Vector | undefined;
/**
* Get a column's vector by name
* @param {string} name
* @returns {Vector | undefined}
*/
  column_by_name(name: string): Vector | undefined;
/**
* Export this RecordBatch to FFI structs according to the Arrow C Data Interface.
*
* This method **does not consume** the RecordBatch, so you must remember to call {@linkcode
* RecordBatch.free} to release the resources. The underlying arrays are reference counted, so
* this method does not copy data, it only prevents the data from being released.
* @returns {FFIRecordBatch}
*/
  toFFI(): FFIRecordBatch;
/**
* Export this RecordBatch to FFI structs according to the Arrow C Data Interface.
*
* This method **does consume** the RecordBatch, so the original RecordBatch will be
* inaccessible after this call. You must still call {@linkcode FFIRecordBatch.free} after
* you've finished using the FFI table.
* @returns {FFIRecordBatch}
*/
  intoFFI(): FFIRecordBatch;
/**
* Consume this RecordBatch and convert to an Arrow IPC Stream buffer
* @returns {Uint8Array}
*/
  intoIPCStream(): Uint8Array;
/**
* The number of columns in this RecordBatch
*/
  readonly numColumns: number;
/**
* The number of rows in this RecordBatch
*/
  readonly numRows: number;
/**
* The {@linkcode Schema} of this RecordBatch.
*/
  readonly schema: Schema;
}
/**
* Metadata for a row group.
*/
export class RowGroupMetaData {
  free(): void;
/**
* Number of rows in this row group.
* @returns {number}
*/
  numRows(): number;
/**
* Number of columns in this row group.
* @returns {number}
*/
  numColumns(): number;
/**
* Returns a single column chunk metadata by index
* @param {number} i
* @returns {ColumnChunkMetaData}
*/
  column(i: number): ColumnChunkMetaData;
/**
* Total byte size of all uncompressed column data in this row group.
* @returns {number}
*/
  totalByteSize(): number;
/**
* Total size of all compressed column data in this row group.
* @returns {number}
*/
  compressedSize(): number;
}
/**
* A named collection of types that defines the column names and types in a RecordBatch or Table
* data structure.
*
* A Schema can also contain extra user-defined metadata either at the Table or Column level.
* Column-level metadata is often used to define [extension
* types](https://arrow.apache.org/docs/format/Columnar.html#extension-types).
*/
export class Schema {
  free(): void;
}
/**
* A schema descriptor. This encapsulates the top-level schemas for all the columns,
* as well as all descriptors for all the primitive columns.
*/
export class SchemaDescriptor {
  free(): void;
/**
* The schemas' name.
* @returns {string}
*/
  name(): string;
/**
* The number of columns in the schema
* @returns {number}
*/
  numColumns(): number;
/**
* The number of fields in the schema
* @returns {number}
*/
  numFields(): number;
}
/**
* A Table in WebAssembly memory conforming to the Apache Arrow spec.
*
* A Table consists of one or more {@linkcode RecordBatch} objects plus a {@linkcode Schema} that
* each RecordBatch conforms to.
*/
export class Table {
  free(): void;
/**
* Access a RecordBatch from the Table by index.
*
* @param index The positional index of the RecordBatch to retrieve.
* @returns a RecordBatch or `null` if out of range.
* @param {number} index
* @returns {RecordBatch | undefined}
*/
  recordBatch(index: number): RecordBatch | undefined;
/**
* Export this Table to FFI structs according to the Arrow C Data Interface.
*
* This method **does not consume** the Table, so you must remember to call {@linkcode
* Table.free} to release the resources. The underlying arrays are reference counted, so
* this method does not copy data, it only prevents the data from being released.
* @returns {FFITable}
*/
  toFFI(): FFITable;
/**
* Export this Table to FFI structs according to the Arrow C Data Interface.
*
* This method **does consume** the Table, so the original Table will be
* inaccessible after this call. You must still call {@linkcode FFITable.free} after
* you've finished using the FFITable.
* @returns {FFITable}
*/
  intoFFI(): FFITable;
/**
* Consume this table and convert to an Arrow IPC Stream buffer
* @returns {Uint8Array}
*/
  intoIPCStream(): Uint8Array;
/**
* Create a table from an Arrow IPC File buffer
* @param {Uint8Array} buf
* @returns {Table}
*/
  static fromIPCFile(buf: Uint8Array): Table;
/**
* Create a table from an Arrow IPC Stream buffer
* @param {Uint8Array} buf
* @returns {Table}
*/
  static fromIPCStream(buf: Uint8Array): Table;
/**
* The number of batches in the Table
*/
  readonly numBatches: number;
/**
* Access the Table's {@linkcode Schema}.
*/
  readonly schema: Schema;
}
/**
* An Arrow vector of unknown type that is guaranteed to have contiguous memory (i.e. cannot be
* chunked).
*/
export class Vector {
  free(): void;
/**
* The data type of this vector
* @returns {DataType}
*/
  data_type(): DataType;
}
/**
* Immutable struct to hold writing configuration for `writeParquet2`.
*
* Use {@linkcode WriterPropertiesBuilder} to create a configuration, then call {@linkcode
* WriterPropertiesBuilder.build} to create an instance of `WriterProperties`.
*/
export class WriterProperties {
  free(): void;
}
/**
* Builder to create a writing configuration for `writeParquet2`
*
* Call {@linkcode build} on the finished builder to create an immputable {@linkcode WriterProperties} to pass to `writeParquet2`
*/
export class WriterPropertiesBuilder {
  free(): void;
/**
* Returns default state of the builder.
*/
  constructor();
/**
* Finalizes the configuration and returns immutable writer properties struct.
* @returns {WriterProperties}
*/
  build(): WriterProperties;
/**
* Sets writer version.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setWriterVersion(value: number): WriterPropertiesBuilder;
/**
* Sets encoding for any column.
*
* If dictionary is not enabled, this is treated as a primary encoding for all
* columns. In case when dictionary is enabled for any column, this value is
* considered to be a fallback encoding for that column.
*
* Panics if user tries to set dictionary encoding here, regardless of dictionary
* encoding flag being set.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setEncoding(value: number): WriterPropertiesBuilder;
/**
* Sets compression codec for any column.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setCompression(value: number): WriterPropertiesBuilder;
/**
* Sets flag to enable/disable statistics for any column.
* @param {boolean} value
* @returns {WriterPropertiesBuilder}
*/
  setStatisticsEnabled(value: boolean): WriterPropertiesBuilder;
}
