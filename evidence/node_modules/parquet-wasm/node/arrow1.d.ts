/* tslint:disable */
/* eslint-disable */
/**
* Read a Parquet file into Arrow data using the [`arrow`](https://crates.io/crates/arrow) and
* [`parquet`](https://crates.io/crates/parquet) Rust crates.
*
* This returns an Arrow table in WebAssembly memory. To transfer the Arrow table to JavaScript
* memory you have two options:
*
* - (Easier): Call {@linkcode Table.intoIPCStream} to construct a buffer that can be parsed with
*   Arrow JS's `tableFromIPC` function.
* - (More performant but bleeding edge): Call {@linkcode Table.intoFFI} to construct a data
*   representation that can be parsed zero-copy from WebAssembly with
*   [arrow-js-ffi](https://github.com/kylebarron/arrow-js-ffi).
*
* Example:
*
* ```js
* import { tableFromIPC } from "apache-arrow";
* // Edit the `parquet-wasm` import as necessary
* import { readParquet } from "parquet-wasm/node/arrow1";
*
* const resp = await fetch("https://example.com/file.parquet");
* const parquetUint8Array = new Uint8Array(await resp.arrayBuffer());
* const arrowWasmTable = readParquet(parquetUint8Array);
* const arrowTable = tableFromIPC(arrowWasmTable.intoIPCStream());
* ```
*
* @param parquet_file Uint8Array containing Parquet data
* @param {Uint8Array} parquet_file
* @returns {Table}
*/
export function readParquet(parquet_file: Uint8Array): Table;
/**
* Write Arrow data to a Parquet file using the [`arrow`](https://crates.io/crates/arrow) and
* [`parquet`](https://crates.io/crates/parquet) Rust crates.
*
* For example, to create a Parquet file with Snappy compression:
*
* ```js
* import { tableToIPC } from "apache-arrow";
* // Edit the `parquet-wasm` import as necessary
* import {
*   Table,
*   WriterPropertiesBuilder,
*   Compression,
*   writeParquet,
* } from "parquet-wasm/node/arrow1";
*
* // Given an existing arrow JS table under `table`
* const wasmTable = Table.fromIPCStream(tableToIPC(table, "stream"));
* const writerProperties = new WriterPropertiesBuilder()
*   .setCompression(Compression.SNAPPY)
*   .build();
* const parquetUint8Array = writeParquet(wasmTable, writerProperties);
* ```
*
* If `writerProperties` is not provided or is `null`, the default writer properties will be used.
* This is equivalent to `new WriterPropertiesBuilder().build()`.
*
* @param table A {@linkcode Table} representation in WebAssembly memory.
* @param writer_properties (optional) Configuration for writing to Parquet. Use the {@linkcode
* WriterPropertiesBuilder} to build a writing configuration, then call `.build()` to create an
* immutable writer properties to pass in here.
* @returns Uint8Array containing written Parquet data.
* @param {Table} table
* @param {WriterProperties | undefined} writer_properties
* @returns {Uint8Array}
*/
export function writeParquet(table: Table, writer_properties?: WriterProperties): Uint8Array;
/**
* @param {string} url
* @param {number | undefined} content_length
* @returns {Promise<ReadableStream>}
*/
export function readParquetStream(url: string, content_length?: number): Promise<ReadableStream>;
/**
* Returns a handle to this wasm instance's `WebAssembly.Memory`
* @returns {any}
*/
export function wasmMemory(): any;
/**
* Returns a handle to this wasm instance's `WebAssembly.Table` which is the indirect function
* table used by Rust
* @returns {any}
*/
export function _functionTable(): any;
/**
* Controls the level of statistics to be computed by the writer
*/
export enum EnabledStatistics {
/**
* Compute no statistics
*/
  None = 0,
/**
* Compute chunk-level statistics but not page-level
*/
  Chunk = 1,
/**
* Compute page-level and chunk-level statistics
*/
  Page = 2,
}
/**
* Supported compression algorithms.
*
* Codecs added in format version X.Y can be read by readers based on X.Y and later.
* Codec support may vary between readers based on the format version and
* libraries available at runtime.
*/
export enum Compression {
  UNCOMPRESSED = 0,
  SNAPPY = 1,
  GZIP = 2,
  BROTLI = 3,
/**
* @deprecated as of Parquet 2.9.0.
* Switch to LZ4_RAW
*/
  LZ4 = 4,
  ZSTD = 5,
  LZ4_RAW = 6,
}
/**
* Encodings supported by Parquet.
* Not all encodings are valid for all types. These enums are also used to specify the
* encoding of definition and repetition levels.
*/
export enum Encoding {
/**
* Default byte encoding.
* - BOOLEAN - 1 bit per value, 0 is false; 1 is true.
* - INT32 - 4 bytes per value, stored as little-endian.
* - INT64 - 8 bytes per value, stored as little-endian.
* - FLOAT - 4 bytes per value, stored as little-endian.
* - DOUBLE - 8 bytes per value, stored as little-endian.
* - BYTE_ARRAY - 4 byte length stored as little endian, followed by bytes.
* - FIXED_LEN_BYTE_ARRAY - just the bytes are stored.
*/
  PLAIN = 0,
/**
* **Deprecated** dictionary encoding.
*
* The values in the dictionary are encoded using PLAIN encoding.
* Since it is deprecated, RLE_DICTIONARY encoding is used for a data page, and
* PLAIN encoding is used for dictionary page.
*/
  PLAIN_DICTIONARY = 1,
/**
* Group packed run length encoding.
*
* Usable for definition/repetition levels encoding and boolean values.
*/
  RLE = 2,
/**
* Bit packed encoding.
*
* This can only be used if the data has a known max width.
* Usable for definition/repetition levels encoding.
*/
  BIT_PACKED = 3,
/**
* Delta encoding for integers, either INT32 or INT64.
*
* Works best on sorted data.
*/
  DELTA_BINARY_PACKED = 4,
/**
* Encoding for byte arrays to separate the length values and the data.
*
* The lengths are encoded using DELTA_BINARY_PACKED encoding.
*/
  DELTA_LENGTH_BYTE_ARRAY = 5,
/**
* Incremental encoding for byte arrays.
*
* Prefix lengths are encoded using DELTA_BINARY_PACKED encoding.
* Suffixes are stored using DELTA_LENGTH_BYTE_ARRAY encoding.
*/
  DELTA_BYTE_ARRAY = 6,
/**
* Dictionary encoding.
*
* The ids are encoded using the RLE encoding.
*/
  RLE_DICTIONARY = 7,
/**
* Encoding for floating-point data.
*
* K byte-streams are created where K is the size in bytes of the data type.
* The individual bytes of an FP value are scattered to the corresponding stream and
* the streams are concatenated.
* This itself does not reduce the size of the data but can lead to better compression
* afterwards.
*/
  BYTE_STREAM_SPLIT = 8,
}
/**
* The Parquet version to use when writing
*/
export enum WriterVersion {
  V1 = 0,
  V2 = 1,
}
/**
* A representation of an Arrow RecordBatch in WebAssembly memory exposed as FFI-compatible
* structs through the Arrow C Data Interface.
*/
export class FFIRecordBatch {
  free(): void;
/**
* Access the pointer to the
* [`ArrowArray`](https://arrow.apache.org/docs/format/CDataInterface.html#structure-definitions)
* struct. This can be viewed or copied (without serialization) to an Arrow JS `RecordBatch` by
* using [`arrow-js-ffi`](https://github.com/kylebarron/arrow-js-ffi). You can access the
* [`WebAssembly.Memory`](https://developer.mozilla.org/en-US/docs/WebAssembly/JavaScript_interface/Memory)
* instance by using {@linkcode wasmMemory}.
*
* **Example**:
*
* ```ts
* import { parseRecordBatch } from "arrow-js-ffi";
*
* const wasmRecordBatch: FFIRecordBatch = ...
* const wasmMemory: WebAssembly.Memory = wasmMemory();
*
* // Pass `true` to copy arrays across the boundary instead of creating views.
* const jsRecordBatch = parseRecordBatch(
*   wasmMemory.buffer,
*   wasmRecordBatch.arrayAddr(),
*   wasmRecordBatch.schemaAddr(),
*   true
* );
* ```
* @returns {number}
*/
  arrayAddr(): number;
/**
* Access the pointer to the
* [`ArrowSchema`](https://arrow.apache.org/docs/format/CDataInterface.html#structure-definitions)
* struct. This can be viewed or copied (without serialization) to an Arrow JS `Field` by
* using [`arrow-js-ffi`](https://github.com/kylebarron/arrow-js-ffi). You can access the
* [`WebAssembly.Memory`](https://developer.mozilla.org/en-US/docs/WebAssembly/JavaScript_interface/Memory)
* instance by using {@linkcode wasmMemory}.
*
* **Example**:
*
* ```ts
* import { parseRecordBatch } from "arrow-js-ffi";
*
* const wasmRecordBatch: FFIRecordBatch = ...
* const wasmMemory: WebAssembly.Memory = wasmMemory();
*
* // Pass `true` to copy arrays across the boundary instead of creating views.
* const jsRecordBatch = parseRecordBatch(
*   wasmMemory.buffer,
*   wasmRecordBatch.arrayAddr(),
*   wasmRecordBatch.schemaAddr(),
*   true
* );
* ```
* @returns {number}
*/
  schemaAddr(): number;
}
/**
* A representation of an Arrow Table in WebAssembly memory exposed as FFI-compatible
* structs through the Arrow C Data Interface.
*/
export class FFITable {
  free(): void;
/**
* Get the total number of record batches in the table
* @returns {number}
*/
  numBatches(): number;
/**
* Get the pointer to one ArrowSchema FFI struct
* @returns {number}
*/
  schemaAddr(): number;
/**
* Get the pointer to one ArrowArray FFI struct for a given chunk index and column index
*
* Access the pointer to one
* [`ArrowArray`](https://arrow.apache.org/docs/format/CDataInterface.html#structure-definitions)
* struct representing one of the internal `RecordBatch`es. This can be viewed or copied (without serialization) to an Arrow JS `RecordBatch` by
* using [`arrow-js-ffi`](https://github.com/kylebarron/arrow-js-ffi). You can access the
* [`WebAssembly.Memory`](https://developer.mozilla.org/en-US/docs/WebAssembly/JavaScript_interface/Memory)
* instance by using {@linkcode wasmMemory}.
*
* **Example**:
*
* ```ts
* import * as arrow from "apache-arrow";
* import { parseRecordBatch } from "arrow-js-ffi";
*
* const wasmTable: FFITable = ...
* const wasmMemory: WebAssembly.Memory = wasmMemory();
*
* const jsBatches: arrow.RecordBatch[] = []
* for (let i = 0; i < wasmTable.numBatches(); i++) {
*   // Pass `true` to copy arrays across the boundary instead of creating views.
*   const jsRecordBatch = parseRecordBatch(
*     wasmMemory.buffer,
*     wasmTable.arrayAddr(i),
*     wasmTable.schemaAddr(),
*     true
*   );
*   jsBatches.push(jsRecordBatch);
* }
* const jsTable = new arrow.Table(jsBatches);
* ```
*
* @param chunk number The chunk index to use
* @returns number pointer to an ArrowArray FFI struct in Wasm memory
* @param {number} chunk
* @returns {number}
*/
  arrayAddr(chunk: number): number;
/**
*/
  drop(): void;
}
/**
*/
export class IntoUnderlyingByteSource {
  free(): void;
/**
* @param {any} controller
*/
  start(controller: any): void;
/**
* @param {any} controller
* @returns {Promise<any>}
*/
  pull(controller: any): Promise<any>;
/**
*/
  cancel(): void;
/**
*/
  readonly autoAllocateChunkSize: number;
/**
*/
  readonly type: string;
}
/**
*/
export class IntoUnderlyingSink {
  free(): void;
/**
* @param {any} chunk
* @returns {Promise<any>}
*/
  write(chunk: any): Promise<any>;
/**
* @returns {Promise<any>}
*/
  close(): Promise<any>;
/**
* @param {any} reason
* @returns {Promise<any>}
*/
  abort(reason: any): Promise<any>;
}
/**
*/
export class IntoUnderlyingSource {
  free(): void;
/**
* @param {any} controller
* @returns {Promise<any>}
*/
  pull(controller: any): Promise<any>;
/**
*/
  cancel(): void;
}
/**
* Raw options for [`pipeTo()`](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/pipeTo).
*/
export class PipeOptions {
  free(): void;
/**
*/
  readonly preventAbort: boolean;
/**
*/
  readonly preventCancel: boolean;
/**
*/
  readonly preventClose: boolean;
/**
*/
  readonly signal: AbortSignal | undefined;
}
/**
*/
export class QueuingStrategy {
  free(): void;
/**
*/
  readonly highWaterMark: number;
}
/**
* Raw options for [`getReader()`](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream/getReader).
*/
export class ReadableStreamGetReaderOptions {
  free(): void;
/**
*/
  readonly mode: any;
}
/**
* A group of columns of equal length in WebAssembly memory with an associated {@linkcode Schema}.
*/
export class RecordBatch {
  free(): void;
/**
* Export this RecordBatch to FFI structs according to the Arrow C Data Interface.
*
* This method **does not consume** the RecordBatch, so you must remember to call {@linkcode
* RecordBatch.free} to release the resources. The underlying arrays are reference counted, so
* this method does not copy data, it only prevents the data from being released.
* @returns {FFIRecordBatch}
*/
  toFFI(): FFIRecordBatch;
/**
* Export this RecordBatch to FFI structs according to the Arrow C Data Interface.
*
* This method **does consume** the RecordBatch, so the original RecordBatch will be
* inaccessible after this call. You must still call {@linkcode FFIRecordBatch.free} after
* you've finished using the FFIRecordBatch.
* @returns {FFIRecordBatch}
*/
  intoFFI(): FFIRecordBatch;
/**
* Consume this RecordBatch and convert to an Arrow IPC Stream buffer
* @returns {Uint8Array}
*/
  intoIPCStream(): Uint8Array;
/**
* The number of columns in this RecordBatch.
*/
  readonly numColumns: number;
/**
* The number of rows in this RecordBatch.
*/
  readonly numRows: number;
/**
* The {@linkcode Schema} of this RecordBatch.
*/
  readonly schema: Schema;
}
/**
* A named collection of types that defines the column names and types in a RecordBatch or Table
* data structure.
*
* A Schema can also contain extra user-defined metadata either at the Table or Column level.
* Column-level metadata is often used to define [extension
* types](https://arrow.apache.org/docs/format/Columnar.html#extension-types).
*/
export class Schema {
  free(): void;
}
/**
* A Table in WebAssembly memory conforming to the Apache Arrow spec.
*
* A Table consists of one or more {@linkcode RecordBatch} objects plus a {@linkcode Schema} that
* each RecordBatch conforms to.
*/
export class Table {
  free(): void;
/**
* Access a RecordBatch from the Table by index.
*
* @param index The positional index of the RecordBatch to retrieve.
* @returns a RecordBatch or `null` if out of range.
* @param {number} index
* @returns {RecordBatch | undefined}
*/
  recordBatch(index: number): RecordBatch | undefined;
/**
* Export this Table to FFI structs according to the Arrow C Data Interface.
*
* This method **does not consume** the Table, so you must remember to call {@linkcode
* Table.free} to release the resources. The underlying arrays are reference counted, so
* this method does not copy data, it only prevents the data from being released.
* @returns {FFITable}
*/
  toFFI(): FFITable;
/**
* Export this Table to FFI structs according to the Arrow C Data Interface.
*
* This method **does consume** the Table, so the original Table will be
* inaccessible after this call. You must still call {@linkcode FFITable.free} after
* you've finished using the FFITable.
* @returns {FFITable}
*/
  intoFFI(): FFITable;
/**
* Consume this table and convert to an Arrow IPC Stream buffer
* @returns {Uint8Array}
*/
  intoIPCStream(): Uint8Array;
/**
* Create a table from an Arrow IPC Stream buffer
* @param {Uint8Array} buf
* @returns {Table}
*/
  static fromIPCStream(buf: Uint8Array): Table;
/**
* The number of batches in the Table
*/
  readonly numBatches: number;
/**
* Access the Table's {@linkcode Schema}.
*/
  readonly schema: Schema;
}
/**
* Immutable struct to hold writing configuration for `writeParquet`.
*
* Use {@linkcode WriterPropertiesBuilder} to create a configuration, then call {@linkcode
* WriterPropertiesBuilder.build} to create an instance of `WriterProperties`.
*/
export class WriterProperties {
  free(): void;
}
/**
* Builder to create a writing configuration for `writeParquet`
*
* Call {@linkcode build} on the finished builder to create an immputable {@linkcode WriterProperties} to pass to `writeParquet`
*/
export class WriterPropertiesBuilder {
  free(): void;
/**
* Returns default state of the builder.
*/
  constructor();
/**
* Finalizes the configuration and returns immutable writer properties struct.
* @returns {WriterProperties}
*/
  build(): WriterProperties;
/**
* Sets writer version.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setWriterVersion(value: number): WriterPropertiesBuilder;
/**
* Sets data page size limit.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setDataPageSizeLimit(value: number): WriterPropertiesBuilder;
/**
* Sets dictionary page size limit.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setDictionaryPageSizeLimit(value: number): WriterPropertiesBuilder;
/**
* Sets write batch size.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setWriteBatchSize(value: number): WriterPropertiesBuilder;
/**
* Sets maximum number of rows in a row group.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setMaxRowGroupSize(value: number): WriterPropertiesBuilder;
/**
* Sets "created by" property.
* @param {string} value
* @returns {WriterPropertiesBuilder}
*/
  setCreatedBy(value: string): WriterPropertiesBuilder;
/**
* Sets encoding for any column.
*
* If dictionary is not enabled, this is treated as a primary encoding for all
* columns. In case when dictionary is enabled for any column, this value is
* considered to be a fallback encoding for that column.
*
* Panics if user tries to set dictionary encoding here, regardless of dictionary
* encoding flag being set.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setEncoding(value: number): WriterPropertiesBuilder;
/**
* Sets compression codec for any column.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setCompression(value: number): WriterPropertiesBuilder;
/**
* Sets flag to enable/disable dictionary encoding for any column.
*
* Use this method to set dictionary encoding, instead of explicitly specifying
* encoding in `set_encoding` method.
* @param {boolean} value
* @returns {WriterPropertiesBuilder}
*/
  setDictionaryEnabled(value: boolean): WriterPropertiesBuilder;
/**
* Sets flag to enable/disable statistics for any column.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setStatisticsEnabled(value: number): WriterPropertiesBuilder;
/**
* Sets max statistics size for any column.
* Applicable only if statistics are enabled.
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setMaxStatisticsSize(value: number): WriterPropertiesBuilder;
/**
* Sets encoding for a column.
* Takes precedence over globally defined settings.
*
* If dictionary is not enabled, this is treated as a primary encoding for this
* column. In case when dictionary is enabled for this column, either through
* global defaults or explicitly, this value is considered to be a fallback
* encoding for this column.
*
* Panics if user tries to set dictionary encoding here, regardless of dictionary
* encoding flag being set.
* @param {string} col
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setColumnEncoding(col: string, value: number): WriterPropertiesBuilder;
/**
* Sets compression codec for a column.
* Takes precedence over globally defined settings.
* @param {string} col
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setColumnCompression(col: string, value: number): WriterPropertiesBuilder;
/**
* Sets flag to enable/disable dictionary encoding for a column.
* Takes precedence over globally defined settings.
* @param {string} col
* @param {boolean} value
* @returns {WriterPropertiesBuilder}
*/
  setColumnDictionaryEnabled(col: string, value: boolean): WriterPropertiesBuilder;
/**
* Sets flag to enable/disable statistics for a column.
* Takes precedence over globally defined settings.
* @param {string} col
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setColumnStatisticsEnabled(col: string, value: number): WriterPropertiesBuilder;
/**
* Sets max size for statistics for a column.
* Takes precedence over globally defined settings.
* @param {string} col
* @param {number} value
* @returns {WriterPropertiesBuilder}
*/
  setColumnMaxStatisticsSize(col: string, value: number): WriterPropertiesBuilder;
}
